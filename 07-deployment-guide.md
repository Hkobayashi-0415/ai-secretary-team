# AIç§˜æ›¸ãƒãƒ¼ãƒ ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  - ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰

**ä½œæˆæ—¥**: 2025å¹´8æœˆ17æ—¥  
**ä½œæˆè€…**: ä¸­é‡äº”æœˆï¼ˆClaude Codeï¼‰  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¦‚è¦

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç’°å¢ƒ
- **é–‹ç™ºç’°å¢ƒ**: ãƒ­ãƒ¼ã‚«ãƒ«Dockerç’°å¢ƒ
- **ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ**: ã‚¯ãƒ©ã‚¦ãƒ‰ä»®æƒ³ãƒã‚·ãƒ³
- **æœ¬ç•ªç’°å¢ƒ**: ã‚¯ãƒ©ã‚¦ãƒ‰ä»®æƒ³ãƒã‚·ãƒ³ + CDN

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥
- **Blue-Green ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ**: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚¼ãƒ­
- **è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯**: å•é¡Œç™ºç”Ÿæ™‚ã®è‡ªå‹•å¾©æ—§
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå¾Œã®è‡ªå‹•æ¤œè¨¼

## ğŸ—ï¸ ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ

### é–‹ç™ºç’°å¢ƒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ãƒ­ãƒ¼ã‚«ãƒ«PC                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Frontend  â”‚  â”‚    Backend      â”‚  â”‚
â”‚  â”‚   (React)   â”‚  â”‚   (FastAPI)     â”‚  â”‚
â”‚  â”‚   Port:3000 â”‚  â”‚   Port:8000     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚     Redis       â”‚  â”‚
â”‚  â”‚   Port:5432 â”‚  â”‚   Port:6379     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æœ¬ç•ªç’°å¢ƒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cloud Provider                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   CDN       â”‚  â”‚   Load      â”‚  â”‚   Application   â”‚    â”‚
â”‚  â”‚  (CloudFlare)â”‚  â”‚  Balancer   â”‚  â”‚    Servers      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Database  â”‚  â”‚   Cache     â”‚  â”‚   File Storage  â”‚    â”‚
â”‚  â”‚ (PostgreSQL)â”‚  â”‚   (Redis)   â”‚  â”‚    (S3/OSS)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †

### 1. é–‹ç™ºç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

#### å‰ææ¡ä»¶
- Docker 20.10+
- Docker Compose 2.0+
- Git 2.30+

#### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †
```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-org/ai-secretary-team.git
cd ai-secretary-team

# 2. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
cp .env.example .env
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦APIã‚­ãƒ¼ã‚’è¨­å®š

# 3. é–‹ç™ºç’°å¢ƒã®èµ·å‹•
make dev-desktop

# 4. ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
curl http://localhost:8000/health
open http://localhost:3000
```

#### ç’°å¢ƒåˆ¥èµ·å‹•ã‚³ãƒãƒ³ãƒ‰
```bash
# ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—VMç”¨ï¼ˆæ¨å¥¨ï¼‰
make dev-desktop

# WSLç”¨ï¼ˆè»½é‡ï¼‰
make dev-wsl

# ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆç”¨ï¼ˆå…±æœ‰å¯¾å¿œï¼‰
make dev-tablet
```

### 2. ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

#### å‰ææ¡ä»¶
- ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- Terraform 1.0+
- Ansible 2.9+

#### ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰
```bash
# 1. ã‚¤ãƒ³ãƒ•ãƒ©ã®æ§‹ç¯‰
cd infrastructure/terraform
terraform init
terraform plan -var-file=staging.tfvars
terraform apply -var-file=staging.tfvars

# 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ—ãƒ­ã‚¤
cd ../ansible
ansible-playbook -i staging/inventory deploy.yml
```

#### ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒè¨­å®š
```yaml
# infrastructure/ansible/staging/group_vars/all.yml
environment: staging
domain: staging.ai-secretary.local
database_url: postgresql://user:pass@staging-db:5432/ai_secretary
redis_url: redis://staging-redis:6379
gemini_api_key: "{{ vault_gemini_api_key }}"
```

### 3. æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

#### å‰ææ¡ä»¶
- ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§ã®å‹•ä½œç¢ºèªå®Œäº†
- æœ¬ç•ªç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æº–å‚™
- SSLè¨¼æ˜æ›¸ã®å–å¾—

#### æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †
```bash
# 1. æœ¬ç•ªç’°å¢ƒã®æ§‹ç¯‰
cd infrastructure/terraform
terraform plan -var-file=production.tfvars
terraform apply -var-file=production.tfvars

# 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ—ãƒ­ã‚¤
cd ../ansible
ansible-playbook -i production/inventory deploy.yml

# 3. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl https://api.ai-secretary.local/health
```

## ğŸ”§ CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### GitHub Actionsè¨­å®š

#### ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ (.github/workflows/main.yml)
```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ai-secretary-team

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        cd backend
        pytest tests/
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Login to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push backend image
      uses: docker/build-push-action@v5
      with:
        context: ./backend
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:${{ github.sha }}
    
    - name: Build and push frontend image
      uses: docker/build-push-action@v5
      with:
        context: ./frontend
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ github.sha }}

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.STAGING_HOST }}
        username: ${{ secrets.STAGING_USER }}
        key: ${{ secrets.STAGING_SSH_KEY }}
        script: |
          cd /opt/ai-secretary-team
          git pull origin develop
          docker-compose -f docker-compose.staging.yml pull
          docker-compose -f docker-compose.staging.yml up -d
          docker system prune -f

  deploy-production:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.PRODUCTION_HOST }}
        username: ${{ secrets.PRODUCTION_USER }}
        key: ${{ secrets.PRODUCTION_SSH_KEY }}
        script: |
          cd /opt/ai-secretary-team
          git pull origin main
          docker-compose -f docker-compose.production.yml pull
          docker-compose -f docker-compose.production.yml up -d
          docker system prune -f
```

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

#### ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ (docker-compose.staging.yml)
```yaml
version: '3.8'

services:
  backend:
    image: ghcr.io/ai-secretary-team/backend:latest
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENVIRONMENT=staging
      - CORS_ORIGINS=https://staging.ai-secretary.local
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  frontend:
    image: ghcr.io/ai-secretary-team/frontend:latest
    environment:
      - VITE_API_URL=https://api-staging.ai-secretary.local
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped

  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=ai_secretary
      - POSTGRES_USER=ai_secretary_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

#### æœ¬ç•ªç’°å¢ƒ (docker-compose.production.yml)
```yaml
version: '3.8'

services:
  backend:
    image: ghcr.io/ai-secretary-team/backend:latest
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENVIRONMENT=production
      - CORS_ORIGINS=https://ai-secretary.local
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  frontend:
    image: ghcr.io/ai-secretary-team/frontend:latest
    environment:
      - VITE_API_URL=https://api.ai-secretary.local
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'

  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=ai_secretary
      - POSTGRES_USER=ai_secretary_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### SSL/TLSè¨­å®š

#### Nginxè¨­å®š (nginx.conf)
```nginx
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }
    
    upstream frontend {
        server frontend:3000;
    }
    
    server {
        listen 80;
        server_name ai-secretary.local;
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name ai-secretary.local;
        
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
        ssl_prefer_server_ciphers off;
        
        location /api/ {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

### ç’°å¢ƒå¤‰æ•°ç®¡ç†

#### ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†
```bash
# æœ¬ç•ªç’°å¢ƒç”¨ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ
export DATABASE_URL="postgresql://user:pass@prod-db:5432/ai_secretary"
export REDIS_URL="redis://prod-redis:6379"
export GEMINI_API_KEY="your_production_gemini_api_key"
export JWT_SECRET_KEY="your_jwt_secret_key"
export ENCRYPTION_KEY="your_encryption_key"
```

#### Vaultè¨­å®š
```yaml
# vault/secrets/production.yml
database_url: "postgresql://user:pass@prod-db:5432/ai_secretary"
redis_url: "redis://prod-redis:6379"
gemini_api_key: "your_production_gemini_api_key"
jwt_secret_key: "your_jwt_secret_key"
encryption_key: "your_encryption_key"
```

## ğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚°

### ç›£è¦–è¨­å®š

#### Prometheusè¨­å®š (monitoring/prometheus.yml)
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'ai-secretary-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'ai-secretary-frontend'
    static_configs:
      - targets: ['frontend:3000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
    scrape_interval: 30s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 30s
```

#### Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
```json
{
  "dashboard": {
    "title": "AI Secretary Team Platform",
    "panels": [
      {
        "title": "API Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])"
          }
        ]
      },
      {
        "title": "Database Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends"
          }
        ]
      }
    ]
  }
}
```

### ãƒ­ã‚°ç®¡ç†

#### ãƒ­ã‚°è¨­å®š (logging/logback.xml)
```xml
<configuration>
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/ai-secretary.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/ai-secretary.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="FILE" />
    </root>
</configuration>
```

## ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

#### è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (scripts/backup.sh)
```bash
#!/bin/bash

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
BACKUP_DIR="/backups/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="ai_secretary"
RETENTION_DAYS=30

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p $BACKUP_DIR

# ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_dump -h localhost -U ai_secretary_user -d $DB_NAME \
    --format=custom --compress=9 \
    --file="$BACKUP_DIR/ai_secretary_$DATE.dump"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼
if [ $? -eq 0 ]; then
    echo "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ: $BACKUP_DIR/ai_secretary_$DATE.dump"
else
    echo "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ"
    exit 1
fi

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤
find $BACKUP_DIR -name "*.dump" -mtime +$RETENTION_DAYS -delete

# S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
if [ -n "$S3_BUCKET" ]; then
    aws s3 cp "$BACKUP_DIR/ai_secretary_$DATE.dump" "s3://$S3_BUCKET/backups/"
fi
```

#### å¾©æ—§æ‰‹é †
```bash
# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
ls -la /backups/postgresql/

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¾©æ—§
pg_restore -h localhost -U ai_secretary_user -d ai_secretary \
    --clean --if-exists \
    /backups/postgresql/ai_secretary_20250817_103000.dump

# 3. å¾©æ—§ã®ç¢ºèª
psql -h localhost -U ai_secretary_user -d ai_secretary -c "SELECT COUNT(*) FROM users;"
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

#### ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
APP_DATA_DIR="/opt/ai-secretary-team/data"
BACKUP_DIR="/backups/app-data"
DATE=$(date +%Y%m%d_%H%M%S)

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
tar -czf "$BACKUP_DIR/app-data_$DATE.tar.gz" -C $APP_DATA_DIR .

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šï¼‰
find $BACKUP_DIR -name "app-data_*.tar.gz" -mtime +30 -delete
```

## ğŸš¨ éšœå®³å¯¾å¿œ

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

#### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (scripts/health-check.sh)
```bash
#!/bin/bash

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
API_URL="http://localhost:8000/health"
FRONTEND_URL="http://localhost:3000"
ALERT_EMAIL="admin@ai-secretary.local"

# APIãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
api_status=$(curl -s -o /dev/null -w "%{http_code}" $API_URL)
if [ $api_status -ne 200 ]; then
    echo "API is down (HTTP $api_status)" | mail -s "AI Secretary Alert" $ALERT_EMAIL
    exit 1
fi

# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
frontend_status=$(curl -s -o /dev/null -w "%{http_code}" $FRONTEND_URL)
if [ $frontend_status -ne 200 ]; then
    echo "Frontend is down (HTTP $frontend_status)" | mail -s "AI Secretary Alert" $ALERT_EMAIL
    exit 1
fi

echo "All services are healthy"
```

### è‡ªå‹•å¾©æ—§

#### è‡ªå‹•å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (scripts/auto-recovery.sh)
```bash
#!/bin/bash

# è‡ªå‹•å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
LOG_FILE="/var/log/ai-secretary-recovery.log"

# ãƒ­ã‚°é–¢æ•°
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" >> $LOG_FILE
}

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
restart_services() {
    log "Restarting services..."
    cd /opt/ai-secretary-team
    docker-compose -f docker-compose.production.yml restart
    log "Services restarted"
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
log "Starting health check and recovery process"

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
if ! ./scripts/health-check.sh; then
    log "Health check failed, attempting recovery"
    restart_services
    
    # å¾©æ—§ç¢ºèª
    sleep 30
    if ./scripts/health-check.sh; then
        log "Recovery successful"
    else
        log "Recovery failed, manual intervention required"
        # ç®¡ç†è€…ã«é€šçŸ¥
        echo "AI Secretary Platform requires manual intervention" | \
            mail -s "CRITICAL: AI Secretary Platform Down" admin@ai-secretary.local
    fi
else
    log "All services are healthy"
fi
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

#### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
```sql
-- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ã‚¯ã‚¨ãƒª
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- æœªä½¿ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç‰¹å®š
SELECT 
    schemaname,
    tablename,
    indexname
FROM pg_stat_user_indexes
WHERE idx_scan = 0;
```

#### ã‚¯ã‚¨ãƒªæœ€é©åŒ–
```sql
-- ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®ç‰¹å®š
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–

#### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
```python
# Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
CACHE_CONFIG = {
    "default_ttl": 3600,  # 1æ™‚é–“
    "max_connections": 100,
    "retry_on_timeout": True,
    "socket_keepalive": True,
    "socket_keepalive_options": {}
}

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®å‘½åè¦å‰‡
CACHE_KEY_PREFIX = "ai_secretary"
CACHE_KEYS = {
    "user": f"{CACHE_KEY_PREFIX}:user:{{user_id}}",
    "assistant": f"{CACHE_KEY_PREFIX}:assistant:{{assistant_id}}",
    "conversation": f"{CACHE_KEY_PREFIX}:conversation:{{conversation_id}}"
}
```

## ğŸ”„ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

### è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

#### ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (scripts/rollback.sh)
```bash
#!/bin/bash

# ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
PREVIOUS_VERSION=$1
CURRENT_DIR="/opt/ai-secretary-team"
BACKUP_DIR="/backups/rollback"

if [ -z "$PREVIOUS_VERSION" ]; then
    echo "ä½¿ç”¨æ–¹æ³•: $0 <previous_version>"
    echo "åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³:"
    ls -la $BACKUP_DIR/
    exit 1
fi

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log "Starting rollback to version $PREVIOUS_VERSION"

# ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
log "Creating backup of current version"
cd $CURRENT_DIR
docker-compose -f docker-compose.production.yml down
tar -czf "$BACKUP_DIR/current_$(date +%Y%m%d_%H%M%S).tar.gz" .

# å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
log "Rolling back to version $PREVIOUS_VERSION"
rm -rf $CURRENT_DIR/*
tar -xzf "$BACKUP_DIR/$PREVIOUS_VERSION.tar.gz" -C $CURRENT_DIR

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
log "Restarting services"
cd $CURRENT_DIR
docker-compose -f docker-compose.production.yml up -d

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
log "Performing health check"
sleep 30
if ./scripts/health-check.sh; then
    log "Rollback successful"
else
    log "Rollback failed, manual intervention required"
    exit 1
fi
```

ã“ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚Šã€å®‰å…¨ã§åŠ¹ç‡çš„ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãŒå®Ÿç¾ã§ãã¾ã™ã€‚
