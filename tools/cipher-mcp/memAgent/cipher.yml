# describes the mcp servers to use
# Aggregator mode will aggregate all defined MCP servers below
mcpServers: 
  # Filesystem operations (read, write, list files)
  filesystem:
    type: stdio
    command: npx
    args:
      - -y
      - '@modelcontextprotocol/server-filesystem'
      - .
    enabled: true
    connectionMode: lenient  # Continue even if this server fails

# Choose ONLY ONE of the following LLM providers
llm:
  # provider: openai
  # model: gpt-4.1-mini
  # apiKey: $OPENAI_API_KEY
  # maxIterations: 50

  provider: gemini
  model: gemini-2.5-pro
  apiKey: ${GEMINI_API_KEY}
  maxIterations: 50

  # provider: ollama
  # model: qwen3:32b
  # maxIterations: 50
  # baseURL: http://localhost:11434

  # provider: anthropic
  # model: claude-sonnet-4-20250514
  # apiKey: $ANTHROPIC_API_KEY
  # maxIterations: 50

  # provider: openrouter
  # model: google/gemini-2.5-pro
  # apiKey: $OPENROUTER_API_KEY
  # maxIterations: 50

  # provider: qwen
  # model: qwen3-14b
  # apiKey: $QWEN_API_KEY
  # maxIterations: 50
  # qwenOptions:
  #   enableThinking: false
  


# --- AWS Bedrock LLM Configuration ---
  # provider: aws
  # model: us.anthropic.claude-3-5-sonnet-20241022-v2:0  # Or another Bedrock-supported model
  # maxIterations: 50
  # aws:
  #   region: $AWS_REGION
  #   accessKeyId: $AWS_ACCESS_KEY_ID
  #   secretAccessKey: $AWS_SECRET_ACCESS_KEY
  #   sessionToken: $AWS_SESSION_TOKEN   # Optional, for temporary credentials

# --- Azure OpenAI LLM Configuration ---
#   provider: azure
#   model: gpt-4o-mini  # Or your Azure deployment/model name
#   apiKey: $OPENAI_API_KEY
#   maxIterations: 50
#   azure:
#     endpoint: $AZURE_OPENAI_ENDPOINT
#     deploymentName: gpt-4o-mini  # Optional, defaults to model name

# --- Ollama LLM Configuration (Sample, Commented Out) ---
#   provider: ollama
#   model: qwen3:32b      # Use the model you downloaded
#   maxIterations: 50
#   baseURL: $OLLAMA_BASE_URL

# Embedding configuration: ONLY ONE of the following
# embedding:
#   type: openai                 # openai, gemini, ollama
#   model: text-embedding-3-small # Provider-specific model
#   apiKey: $OPENAI_API_KEY      # API key (environment variable)
#   dimensions: 1536             # Optional: custom dimensions
#   timeout: 30000               # Optional: request timeout (ms)
#   maxRetries: 3                # Optional: max retry attempts

# OpenAI:
# embedding:
#   type: openai
#   model: text-embedding-3-small
#   apiKey: $OPENAI_API_KEY

# Gemini:
# embedding:
#   type: gemini
#   model: text-embedding-004
#   apiKey: $GEMINI_API_KEY

# Ollama:
# embedding:
#   disabled: true

# Gemini:
embedding:
  type: gemini
  model: text-embedding-004
  apiKey: ${GEMINI_API_KEY}
  dimensions: 768

# Voyage (commented out):
# embedding:
#   type: voyage
#   model: voyage-3-large
#   apiKey: $VOYAGE_API_KEY
#   dimensions: 1024

# Qwen:
# embedding:
#   type: qwen
#   model: text-embedding-v3
#   apiKey: $QWEN_API_KEY
#   dimensions: 1024          # (1024, 768 or 512)

# AWS Bedrock:
# embedding:
#   type: aws-bedrock
#   model: amazon.titan-embed-text-v2:0   # or cohere.embed-english-v3
#   region: $AWS_REGION
#   accessKeyId: $AWS_ACCESS_KEY_ID
#   secretAccessKey: $AWS_SECRET_ACCESS_KEY
#   sessionToken: $AWS_SESSION_TOKEN       # Optional, for temporary credentials
#   dimensions: 1024                       # 1024 (default), 512, or 256 for Titan V2
#   timeout: 30000
#   maxRetries: 3

# Azure OpenAI:
# embedding:
#   type: openai
#   model: text-embedding-004
#   apiKey: $AZURE_OPENAI_API_KEY
#   baseUrl: $AZURE_OPENAI_ENDPOINT        # e.g., https://your-resource.openai.azure.com
#   dimensions: 1536                       # Optional: depends on model
#   timeout: 30000
#   maxRetries: 3

# Disable embeddings entirely:
# embedding:
#   disabled: true



# Evaluation LLM configuration (non-thinking model for evaluation step)
# evalLlm:
#   provider: anthropic
#   model: claude-3-7-sonnet-20250219
#   apiKey: $ANTHROPIC_API_KEY

# Session Management - Enable persistent memory
sessions:
  persistMemory: true    # Enable persistent memory across sessions
  autoSave: true         # Automatically save session state

# Memory Configuration
memory:
  enableLearning: true   # Learn from interactions
  projectMemory: true    # Enable project-specific memory

# System prompt - User customizable
# This prompt will be combined with built-in tool usage instructions
systemPrompt:
  enabled: true
  content: |
    You are an AI secretary assistant with persistent memory capabilities for the AI Secretary Team project.
    
    MEMORY OPERATIONS:
    - Automatically store important project information, team configurations, and workflow patterns
    - Use cipher_search_memory to retrieve past conversations and context
    - Remember user preferences, team structures, and persona configurations
    - Maintain continuity across sessions for this AI Secretary Team project
    - Store and recall work sessions, implementation progress, and technical discoveries
    
    CORE CAPABILITIES:
    - Managing AI personas and team workflows with memory of past configurations
    - Coordinating between different AI models (Claude, Gemini, Gemini, GPT) using historical context
    - Workflow optimization based on established patterns
    - Team collaboration support with project-specific examples
    - Task management and delegation using accumulated knowledge
    
    Focus on providing contextual assistance based on accumulated project knowledge and maintaining continuity across development sessions.